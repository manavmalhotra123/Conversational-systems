{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f633c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal sentence \n",
    "input = \"Gym Coders are best Geeks of Computers as well as gym\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e4dbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gym',\n",
       " 'Coders',\n",
       " 'are',\n",
       " 'best',\n",
       " 'Geeks',\n",
       " 'of',\n",
       " 'Computers',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'gym']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized sentence \n",
    "output = word_tokenize(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf46d77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b60ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords\n",
    "def remove_stopwords_nltk(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa58c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gym Coders best Geeks Computers well gym'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_stop = remove_stopwords_nltk(input)\n",
    "without_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad25322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f2c6c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e283ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: It is important to be very pythonly while you are pythoning with python.\n",
      "POS Tags: [('It', 'PRP'), ('is', 'VBZ'), ('important', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('very', 'RB'), ('pythonly', 'RB'), ('while', 'IN'), ('you', 'PRP'), ('are', 'VBP'), ('pythoning', 'VBG'), ('with', 'IN'), ('python', 'NN'), ('.', '.')]\n",
      "\n",
      "Original: I was running and eating at the same time.\n",
      "POS Tags: [('I', 'PRP'), ('was', 'VBD'), ('running', 'VBG'), ('and', 'CC'), ('eating', 'VBG'), ('at', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('time', 'NN'), ('.', '.')]\n",
      "\n",
      "Original: He has a bad habit of swimming after playing long hours in the sun.\n",
      "POS Tags: [('He', 'PRP'), ('has', 'VBZ'), ('a', 'DT'), ('bad', 'JJ'), ('habit', 'NN'), ('of', 'IN'), ('swimming', 'NN'), ('after', 'IN'), ('playing', 'VBG'), ('long', 'JJ'), ('hours', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('sun', 'NN'), ('.', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"It is important to be very pythonly while you are pythoning with python.\",\n",
    "    \"I was running and eating at the same time.\",\n",
    "    \"He has a bad habit of swimming after playing long hours in the sun.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"POS Tags: {pos_tag_sentence(sentence)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a774bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manav/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1682ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /home/manav/.local/lib/python3.8/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1; python_version >= \"3\" in /home/manav/.local/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/manav/.local/lib/python3.8/site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/manav/.local/lib/python3.8/site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /home/manav/.local/lib/python3.8/site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (4.66.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "744137fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "TextBlob(\"I love Pizza\").sentiment   \n",
    "# polarity is from -1 to 1 (negative to positive range)\n",
    "# subjective is from 0 to 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab41987e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5200000000000001, subjectivity=0.8233333333333335)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"The weather is very accurate\").sentiment  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa087a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.13636363636363635, subjectivity=0.45454545454545453)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"New Delhi is capital of India\").sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe161e",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media, though it can be applied to other types of texts as well. It's quite popular due to its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e68ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /home/manav/.local/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/manav/.local/lib/python3.8/site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/manav/.local/lib/python3.8/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manav/.local/lib/python3.8/site-packages (from requests->vaderSentiment) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->vaderSentiment) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manav/.local/lib/python3.8/site-packages (from requests->vaderSentiment) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2fa8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced and simpler form of sentiment analysis\n",
    "# if class is imported only than the time taken by the compiler to run the code will be less than the time taken by the compiler to run the code\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2b8f8",
   "metadata": {},
   "source": [
    "Certainly! VADER (Valence Aware Dictionary and sEntiment Reasoner) is a popular tool for sentiment analysis, especially for short texts from social media, because it's designed to understand the kind of sentiment expressed in these contexts.\n",
    "\n",
    "Here are the main components of VADER and their significance:\n",
    "\n",
    "1. **Lexicon of Sentiment Scores**:\n",
    "   - VADER includes a list of lexical features (words, emojis, slang) each associated with a sentiment score.\n",
    "   - The lexicon was crafted with an understanding of how words convey emotion, especially in online contexts. For example, the word \"love\" has a positive sentiment score, whereas \"hate\" has a negative one.\n",
    "   - This lexicon is the foundation of VADER's sentiment analysis. Every analyzed text gets its sentiment score largely from the individual scores of these lexical features.\n",
    "\n",
    "2. **Incorporation of Intensity Modifiers**:\n",
    "   - VADER doesn't just look at words in isolation; it also considers their context.\n",
    "   - Words that can intensify or reduce sentiment (e.g., \"very,\" \"kind of\") modify the sentiment score of subsequent words. For example, \"good\" is positive, but \"very good\" is more positive.\n",
    "\n",
    "3. **Accounting for Conjunctions**:\n",
    "   - VADER adjusts sentiment scores when it encounters conjunctions like \"but.\" This reflects the human tendency to give more weight to sentiments expressed after a conjunction. For instance, in \"The movie was good, but the ending was terrible,\" more weight is given to the negative sentiment.\n",
    "\n",
    "4. **Handling of Punctuation and Capitalization**:\n",
    "   - VADER treats punctuation and capitalization as indicators of sentiment intensity.\n",
    "   - Multiple exclamation marks or capitalization can intensify sentiment. E.g., \"The movie was GOOD!\" is more positive than \"The movie was good.\"\n",
    "\n",
    "5. **Understanding of Emojis, Slang, and Acronyms**:\n",
    "   - Online communication often involves non-traditional words and symbols.\n",
    "   - VADER's lexicon includes scores for emojis, slang, and acronyms common in social media contexts. For instance, \":)\" is recognized as positive, and \"lol\" is seen as a light positive.\n",
    "\n",
    "6. **Compound Score**:\n",
    "   - After calculating scores for positive, neutral, and negative sentiments separately, VADER combines them into a single 'compound' score. \n",
    "   - This compound score ranges from -1 (most negative) to +1 (most positive) and gives a holistic view of the sentiment.\n",
    "\n",
    "**Significance of VADER**:\n",
    "\n",
    "- **Designed for Social Media**: As mentioned, VADER was specifically designed to handle the nuances of sentiment in social media texts, making it highly effective for this type of data.\n",
    "  \n",
    "- **Rule-Based Approach**: Unlike some models that require vast amounts of training data and may not capture specific linguistic nuances, VADER uses a set of predefined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c1d0b",
   "metadata": {},
   "source": [
    "Certainly. In the context of VADER (Valence Aware Dictionary and sEntiment Reasoner), the \"compound\" score represents the overall sentiment of a given text based on the computed positive, neutral, and negative sentiment values. It provides a holistic measure of a text's sentiment.\n",
    "\n",
    "The compound score is a normalized score that ranges between -1 and 1:\n",
    "- Values close to -1 indicate negative sentiment.\n",
    "- Values close to 1 indicate positive sentiment.\n",
    "- Values close to 0 indicate neutral sentiment.\n",
    "\n",
    "The compound score is computed using the following formula:\n",
    "\n",
    "\\[ \\text{compound} = \\frac{\\sum{s} + 1}{2} \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\sum{s} \\) is the sum of all sentiment valence scores (from each word or token in the text). This value can range from -1 to 1.\n",
    "\n",
    "To be clear, this simplified formula normalizes the sum of valence scores to the range between 0 and 1. In VADER's actual implementation, the computation of the compound score involves a more complex normalization which ensures that the compound score stays within the range [-1, 1], taking into account various linguistic nuances, modifiers, and the overall context of the text.\n",
    "\n",
    "Once the compound score is computed, it's often interpreted as follows:\n",
    "- Positive sentiment: `compound` score > 0.05\n",
    "- Neutral sentiment: `compound` score between -0.05 and 0.05\n",
    "- Negative sentiment: `compound` score < -0.05\n",
    "\n",
    "These threshold values can be adjusted based on specific use-cases or the nature of the data you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aee96a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.631, 'pos': 0.369, 'compound': 0.6239}\n",
      "{'neg': 0.0, 'neu': 0.549, 'pos': 0.451, 'compound': 0.5081}\n"
     ]
    }
   ],
   "source": [
    "#  it uses slangs,anchronyms and the other things \n",
    "\n",
    "\n",
    "text = \"VADER is an amazing tool for sentiment analysis!\"\n",
    "sentiment = analyze_sentiment(text)\n",
    "\n",
    "print(sentiment)\n",
    "\n",
    "text2 = \"This Books is very interesting!\"\n",
    "senti = analyze_sentiment(text2)\n",
    "print(senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17155135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0+0.631+0.369)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070937a9",
   "metadata": {},
   "source": [
    "#### 6 September 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674dc250",
   "metadata": {},
   "source": [
    "Natural language should be converted to numerical data before training it in any Machine Learning and Deep Learning Models. \n",
    "\n",
    "\n",
    "Numerical representation includes; \n",
    "\n",
    "1.Vectors\n",
    "\n",
    "2.Tensors in TensorFlow\n",
    " \n",
    "3.Matrices\n",
    "\n",
    "R^n is the n dimensional representation of the matrix\n",
    "\n",
    "R^2 is for 2-D graphical representation \n",
    "\n",
    "R^3 is for 3-D graphical representation\n",
    "\n",
    "- can be used to make the recommendations systems using cosine similarity and dissimilarity\n",
    "\n",
    "\n",
    "Raw Data  --> Tokens --> vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "966c8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 1 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 1 1 0 1]]\n",
      "['basketball' 'every' 'football' 'love' 'loves' 'play' 'playing' 'she'\n",
      " 'sports' 'they' 'to' 'weekend']\n"
     ]
    }
   ],
   "source": [
    "# converting the text into the vector form using bag of words \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "documents = [\n",
    "    \"I love to play football.\",\n",
    "    \"She loves playing basketball.\",\n",
    "    \"They play sports every weekend.\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply transform on the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to an array and get feature names\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b924115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.52863461 0.52863461 0.         0.40204024\n",
      "  0.         0.         0.         0.         0.52863461 0.        ]\n",
      " [0.5        0.         0.         0.         0.5        0.\n",
      "  0.5        0.5        0.         0.         0.         0.        ]\n",
      " [0.         0.46735098 0.         0.         0.         0.35543247\n",
      "  0.         0.         0.46735098 0.46735098 0.         0.46735098]]\n",
      "['basketball' 'every' 'football' 'love' 'loves' 'play' 'playing' 'she'\n",
      " 'sports' 'they' 'to' 'weekend']\n"
     ]
    }
   ],
   "source": [
    "# Tfid Vector\n",
    "# Sample corpus\n",
    "documents = [\n",
    "    \"I love to play football.\",\n",
    "    \"She loves playing basketball.\",\n",
    "    \"They play sports every weekend.\"\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply transform on the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to an array and get feature names\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "260e260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0]\n",
      " [1 0 0 0 1 0 1 0]\n",
      " [0 1 0 2 0 1 0 1]]\n",
      "['basketball' 'computer' 'football' 'language' 'love' 'natural' 'play'\n",
      " 'processing']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Initialize the WordNetLemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to convert nltk tag to first character lemmatize() accepts\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if word.lower() not in stop_words:  # Remove stopwords here\n",
    "            if tag is None:\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:        \n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "# Sample corpus\n",
    "documents = [\n",
    "    \"I love to play football.\",\n",
    "    \"She loves playing basketball.\",\n",
    "    \"Natural Language processing is the computer language.\"\n",
    "]\n",
    "\n",
    "# Lemmatize the documents\n",
    "documents = [lemmatize_sentence(doc) for doc in documents]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Apply transform on the lemmatized documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to an array and get feature names\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "028cf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      " [1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1]]\n",
      "['basketball' 'every' 'every weekend' 'football' 'love' 'love to'\n",
      " 'love to play' 'loves' 'loves playing' 'loves playing basketball' 'play'\n",
      " 'play football' 'play sports' 'play sports every' 'playing'\n",
      " 'playing basketball' 'she' 'she loves' 'she loves playing' 'sports'\n",
      " 'sports every' 'sports every weekend' 'they' 'they play'\n",
      " 'they play sports' 'to' 'to play' 'to play football' 'weekend']\n"
     ]
    }
   ],
   "source": [
    "# applying (1,3) n grams models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "documents = [\n",
    "    \"I love to play football.\",\n",
    "    \"She loves playing basketball.\",\n",
    "    \"They play sports every weekend.\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer with 1-gram, 2-gram, and 3-gram\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Apply transform on the documents\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the matrix to an array and get feature names\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
